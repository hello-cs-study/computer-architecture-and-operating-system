# 읽은 범위

> p.60 십육진법 ~ p.89

### 16진법

> -   0과 1로만 모든 숫자를 표현하려면 숫자의 길이가 굉장히 길어짐.
> -   그래서 이진법 외에 16진법도 자주 사용한다.
> -   중요한점) 16진수 한글자는 4bit! (왜냐하면 16진수는 0부터 F까지로 총 16개까지 표현이 가능함 -> 2^4 = 16 -> 4bit)

#### 왜 굳이 16진법을 사용할까? 10진법도 있는데?

> **이진수를 16진수로, 16진수를 2진수로 변환하는게 매우 쉽기 때문이다!**

### 16진수를 2진수로 변환하기

-   `Ox1A2B` -> `Ob0001 / Ob1010 / Ob0010 / Ob1011`
-   `0001101000101011(2)` 가 `Ox1A2B`를 2진수로 표현한 값이다.

### 2진수를 16진수로 변환하기

> -   **2진수 숫자를 4개씩 끊는 것이 제일 중요하다!**
> -   8421을 꼭 외우도록 하자.

-   `Ob11010101` -> `Ob1101 / Ob0101`
-   `0b1101 -> D`
-   `0b0101 -> 5`
-   이 둘을 이어붙이면 `OxD5` 가 되는 것이다.

참고로, 하드웨어와 밀집하게 맞닿아있는 개발 분야에서는 코드에 16진수도 직접 쓰는 경우가 많으니 참고하자.

### 정리

-   `비트(bit)`는 0과 1로 표현할 수 있는 `가장 작은 정보 단위`.
-   `바이트, 킬로바이트, 메가바이트` 등의 단위는 비트보다 더 큰 정보단위.
-   `이진법`은 1을 넘어가는 시점에 자리 올림을 해서 0과 1만으로 수를 표현하는 방법.
-   `이진법에서 음수는 2의 보수`로 표현할 수 있다.
-   `십육진법`은 15를 넘어가는 시점에 자리 올림하여 수를 표현하는 방법.

## 2-2) 0과 1로 문자를 표현하는 방법

> **컴퓨터는 키보드로 입력한 문자들을 실시간으로 모니터에 띄워주는데 어떻게 문자를 이해하고 모니터에 출력하는걸까?**

### 문자 집합과 인코딩

> -   `문자집합`: 컴퓨터가 인식할 수 있는 문자들의 모음
> -   `인코딩`: 문자집합의 문자들을 컴퓨터가 이해할 수 있는 0과 1로 변환하는 과정
> -   `디코딩`: 0과 1로 표현된 문자 코드를 사람이 읽을 수 있는 문자로 변환하는 과정

#### 문자집합 (Character Set)

-   `컴퓨터가 인식하고 표현할 수 있는 문자의 모음`
-   e.g) 문자집합 = { a, b, c, d, e } 인 경우 컴퓨터는 이 5개의 문자만을 이해할 수 있고 f나 g와 같은 문자는 인식하지 못함.
-   물론, 문자집합에 속해있다고 다 이해할 수 있는건 아니다. **0과 1로 변환할 수 있어야 컴퓨터가 이해할 수 있다.**
-   이러한 변환 과정을 `문자 인코딩` 이라고 한다. (인코딩 후 0과 1로 이루어진 결과값이 문자 코드가 된다)
-   인코딩의 반대과정으로 0과 1로 이루어진 문자코드를 사람이 이해할 수 있는 문자로 변환하는 과정이 `문자 디코딩` 이다.

### 아스키 코드 (ASCII)

> -   아스키 문자 집합에 속한 문자는 각각 `7비트`로 표현된다.
> -   7비트이기 때문에 2^7로 총 128개의 문자를 표현할 수 있다.
> -   참고로 하나의 아스키 문자를 나타내기 위해 8비트(1Byte)를 사용하는데 8비트 중 1비트는 패리티 비트(Parity bit) 라고 불리는 오류 검출을 위해 사용되는 비트이다.
> -   그래서 실질적으로 문자 표현을 위해 사용되는 비트는 7비트이다.

**아스키 코드는 매우 간단하게 인코딩된다는 장점이 있지만 한글, 아스키 외의 문자, 특수문자를 표현할 수 없다는 단점이 있음.**
아스키 문자 집합에 속한 문자들은 7비트로 표현하기 때문에 128개보다 많은 문자를 표현하지 못해서 다양한 문자를 표현하기 위해 1비트 추가한 8비트의 `확장 아스키`가 등장했지만
256개의 문자밖에 되지 않아서 부족했다.

### EUC-KR

> -   KS X 1001, KS X 1003 이라는 문자 집합을 기반으로 하는 대표적인 완성형 인코딩 방식
> -   초성, 중성, 종성이 모두 결합된 한글 단어에 `2바이트` 크기의 코드를 부여함.

#### <한글의 특수성>

-   한글은 각 음절 하나하나가 초성,중성,종성의 조합으로 이루어져 있음.
-   한글 인코딩에는 `완성형(한글 완성형 인코딩)`과 `조합형(한글 조합형 인코딩)`이 존재한다.

#### <완성형 인코딩>

-   초성, 중성, 종성의 조합으로 이루어진 완성된 하나의 글자에 고유한 코드를 부여하는 인코딩 방식.
-   예를 들어, `가`는 1, `나`는 2 이런 식으로 인코딩!

#### <조합형 인코딩>

-   초성을 위한 비트열, 중성을 위한 비트열, 종성을 위한 비트열을 할당해서 조합으로 하나의 글자 코드를 완성하는 인코딩 방식.

한글 1글자에는 2바이트 코드가 부여된다. 그러면 16비트가 필요할 것이고 16비트는 4자리의 16진수로 표현할 수 있음. 왜냐하면 16진수 한 글자에 4비트이기 때문이다.

**즉, EUC-KR로 인코딩된 한글은 4자리 16진수로 나타낼 수 있다.**

EUC-KR 인코딩 방식으로는 약 2350개 정도의 한글 단어를 표현할 수 있다. 아스키 코드보다 표현할 수 있는 문자가 많지만 모든 한글 조합을 표현할 수 있는 정도의 양이 안되어서 실제로 은행, 학교 등에서 피해를 받는 사례도 있었다고 한다.
이러한 문제를 해결하기 위해 등장한게 `마이크로소프트의 CP949` 인데, 이 마저도 넉넉한 양은 아니었다.

### 유니코드와 UTF-8

> -   `유니코드`는 **여러 나라의 문자를 광범위하게 표현할 수 있는 문자 집합이다.**
> -   유니코드 글자에 부여된 값 앞에 `U+` 라고 붙는 경우도 있는데 이는 유니코드에서 16진수를 표현할 때의 기법이다.

-   아스키코드나 EUC-KR은 글자에 부여된 값을 그대로 인코딩 값으로 삼았음.
-   UTF-8, UTF-16, UTF-32 등이 있음 (UTF: Unicode Transformation Format)

#### UTF-8

> -   UTF-8은 1바이트부터 4바이트까지의 인코딩 결과를 만들어 낸다.

-   Ox0000 부터 Ox007F 까지는 1바이트로 표현 (0XXXXXXX)
-   Ox0080 부터 Ox07FF 까지는 2바이트로 표현 (110XXXXX 10XXXXXX)
-   Ox0800부터 OxFFFF 까지는 3바이트로 표현 (1110XXXX 10XXXXXX 10XXXXXX)
-   Ox10000부터 Ox10FFFF 까지는 4바이트로 표현 (11110XXX 10XXXXXX 10XXXXXX 10XXXXXX)

예를 들어 '한글'은 '한'에 부여된 값은 0xD55C, '글'에 부여된 값은 0xAE00이다.<br/>
0xD55C는 0x0800부터 0xFFFF 사이에 있고 글도 마찬가지이다.<br/>
**그래서 '한'과 '글'을 UTF-8로 인코딩하면 3바이트로 표현될 것을 예상할 수 있다.**

-   0xD55C = 1101 0101 0101 1100
-   0xAE00 = 1010 1110 0000 0000

3번째 줄에 해당하기 때문에 다음과 같이 표현할 수 있다.

-   0xD55C = 11101101 10010101 10011100
-   0xAE00 = 11101010 10111000 10000000

### 정리

-   `문자집합`: 컴퓨터가 인식할 수 있는 문자의 모음. 문자집합에 속한 문자를 인코딩해서 0과 1로 표현할 수 있음.
-   아스키 문자 집합에서 0부터 127까지의 수가 할당되어 아스키 코드로 할당됨.
-   `EUC-KR`는 한글을 2바이트 크기로 인코딩할 수 있는 완성형 인코딩 방식이다.
-   `유니코드`는 여러 나라의 문자들을 광범위하게 표현할 수 있는 통일된 문자 집합이고, UTF-8, UTF-16, UTF-32는 유니코드 문자의 인코딩 방식이다.

---

# 챕터3) 명령어

## 03-1) 소스코드와 명령어

> -   컴퓨터: 명령어를 처리하는 기계
> -   모든 소스코드는 컴퓨터 내부에서 명령어로 변환된다.

### 고급 언어와 저급 언어

-   프로그래밍 언어는 컴퓨터가 이해하는 언어가 아닌 사람이 이해하고 작성하기 쉽게 만들어진 언이임. 컴퓨터는 이 언어를 이해하지 못함.
-   그래서 사람을 위한 언어를 `고급언어` 라고 한다. 우리가 알고있는 대부분의 프로그래밍 언어는 고급언어이다.
-   컴퓨터가 직접 이해하고 실행할 수 있는 언어를 `저급언어` 라고 한다.
-   **즉, 고급언어로 작성된 소스코드가 실행되려면 반드시 저급 언어인 명령어로 변환되어야 한다.**

#### 저급언어

-   저급언어는 크게 `기계어`와 `어셈블리어`가 있다.
-   기계어는 0과 1의 명령어 비트로 이루어진 언어.
-   그렇지만 기계어를 보면 컴퓨터가 어떻게 작동하는지 이해가 되지 않을 것이다. 그래서 등장한 언어가 `어셈블리어`이다.
-   **다시 말해, 0과 1로 표현된 명령어(기계어)를 읽기 편한 상태로 번역한 언어가 어셈블리어이다.**

```
[기계어] 0101 0101 -> [어셈블리어] push rbp
[기계어] 0101 1101 -> [어셈블리어] pop rbp
[기계어] 1100 0011 -> [어셈블리어] ret
```

### 고급언어는 어떻게 저급언어로 변환될까?

> -   크게 `컴파일` 방식과 `인터프리트` 방식이 있다.
> -   컴파일 방식으로 동작하는 프로그래밍 언어는 `컴파일 언어`, 인터프리트 방식으로 작동하는 프로그래밍 언어는 `인터프리트 언어` 라고 한다.

#### 컴파일 언어

> -   컴파일러에 의해 소스코드 전체가 저급언어로 변환되어 실행되는 고급언어.
> -   대표적으로는 C언어가 있다.

컴파일 언어로 작성된 소스코드는 코드 전체가 저급 언어로 변환되는 과정을 거치는데 이를 `컴파일` 이라고 한다.<br/>
그리고 컴파일을 수행해주는 도구를 `컴파일러` 라고 한다. 컴파일러는 개발자가 작성한 소스코드 전체를 훑으면서 문법적인 오류는 없는지, 실행 가능한 코드인지 등을 따지고 저급언어로 컴파일한다.
이렇게 컴파일러를 통해 저급언어로 변환된 코드를 `목적코드` 라고 한다.

**결론은 컴파일 언어로 작성된 소스코드는 컴파일러에 의해 저급언어로 변환되고(컴파일 과정) 컴파일 결과로 저급 언어인 목적코드가 생성된다.**

#### 인터프리터 언어

> -   인터프리터에 의해 소스코드가 1줄씩 실행되는 고급언어
> -   대표적으로는 Python이 있다.

-   소스코드 전체가 저급언어로 변환되는 컴파일 언어와는 달리 인터프리터 언어는 소스코드를 한줄씩 차례로 실행한다.
-   인터프리터는 소스코드를 한 줄씩 저급언어로 변환해서 실행해주는 도구
-   인터프리터 언어는 컴퓨터와 대화하듯 소스코드를 한줄씩 실행하기 때문에 소스코드 전체를 저급언어로 변환하는 시간을 기다릴 필요가 없다.

그리고 컴파일 언어는 소스코드 내에 오류가 하나라도 있으면 컴파일이 안되었는데, 인터프리터 언어는 소스코드를 한줄씩 실행하기 때문에 소스코드 N번째 줄에 문법 오류가 있더라도 N-1번째 줄까지는 수행된다.

참고로 **인터프리터 언어는 컴파일 언어보다 느리다.**<br/>
컴파일을 통해 나온 결과물인 목적 코드는 컴퓨터가 이해하고 실행할 수 있는 저급언어이지만, 인터프리터 언어는 소스코드 마지막에 이를 때까지 한줄한줄씩 저급언어로 해석하며 실행해야 하기 때문이다.

**그리고 컴파일과 인터프리터를 명확하게 구분할 수는 없음. Java도 저급언어가 되는 과정에서 컴파일과 인터프리터 모두 수행되고, Python도 마찬가지이다.**

### 정리

-   고급언어는 사람이 이해하고 작성하기 쉽게 만들어진 언어이다.
-   저급언어는 컴퓨터가 직접 이해하고 실행할 수 있는 언어이다.
-   저급언어는 0과 1로 이루어진 명령어로 구성된 기계어와 기계어를 사람이 읽기 편한 형태로 번역한 어셈블리어가 있다.
-   컴파일 언어는 컴파일러에 의해 소스코드 전체가 저급언어로 변환되어 실행되는 언어이다. 목적코드
-   인터프리터 언어는 인터프리터에 의해 소스코드가 한줄한줄씩 저급언어로 변환되어 실행되는 언어이다.
